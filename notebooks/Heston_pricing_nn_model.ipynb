{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Network Model for Learning Heston Pricing Simulation method\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  10 minutes |\n",
    "|**Packages used** | Defined with relevant DocStrings within repository   |\n",
    "|**Outcome** | Trained NN model save in the directory ./model/output | \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step is to import all relevant packages to train neural network model. \n",
    "\n",
    "### Note that in the repository under the directory ./model/implied_volatility/core/, I have created a module heston_model_pricing which contains get_heston_price method that generates simultaneous simulation of stock price and volatility process to compute option price under heston model using numerical method. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although majority of option parameters require to compute heston price are self-explanatory, some of them that are explicitely required for evolution of stochastic process and compute price using numerical method are as follows:- \n",
    "\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Initial variance** |  Starting variance at the start of simulation period |\n",
    "|**Long average variance** | Long term average variance of underlying volatility |\n",
    "|**Reversion speed** | Volatility evolution under Stochastic process follows mean-reverting process, speed signifies how strong it is|\n",
    "|**Correlation** | Correlation among underlying stochastic price and volatility evolution weiner process, used to get random value from the joint distribution|\n",
    "|**Volatility of the variance** | Used for simulation volatility evolution process |\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # just to remove clutter in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.pricing.utils.get_data import get_current_price, pre_processing\n",
    "from model.pricing.core.neural_network import run_nn_model\n",
    "from model.implied_volatility.core.heston_model_pricing import create_heston_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step does the following:- \n",
    "\n",
    "- Define data size, read ticker information, and retrieve current price for that ticker. As an example, I have used S&P500\n",
    "- Create a diversified set of input parameters using Latin Hypercube sampling to cover range of possible input parameters. This model contains wide range of input parameters as compare to building BS pricing nn model as it also contains parameters as explined above\n",
    "- Compute heston price using numerical method and then compute implied volatility using brent method which is derivative free as Newton Raphson method generally blows up as vega is close to 0 for deep OTM and ITM options. \n",
    "- Read already tuned hyperparmeters. Method used to identify these hyper parameters is Random search using cross-validation, same hyper parameters are used as per BS neural network model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_size = 10000\n",
    "df, st_current_price, range_of_inputs = pre_processing(data_set_size, \"HESTON\")\n",
    "dt_set = create_heston_dataset(df, st_current_price, range_of_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_hyper_parameter_file_name = os.getcwd() + \"/model/output/\" + \"best_hyper_parameter.json\"\n",
    "with open(tuned_hyper_parameter_file_name) as f:\n",
    "    df_hyper = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step does the following:- \n",
    "\n",
    "- Define columns used as input features and the target variable, much more input features as compare to BS nn model.\n",
    "- Run the model using the cycilical decaying learning rate, more detail is avaiable in the DocString of run_nn_model method\n",
    "- Plot the loss on the training set and validation with each epoch\n",
    "- The below plot demonstrates that model is not overfitting as loss is decreasing in both training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['moneyness', 'time_to_maturity', 'risk_free_rate', 'correlation', 'reversion_speed',\n",
    "                       'Long_average_variance', 'vol_vol', 'initial_variance']\n",
    "target = 'opt_price_by_strike'\n",
    "model, training_history = run_nn_model(dt_set, df_hyper, feature_columns, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_mse = [np.log(i) for i in training_history.history['mse']]\n",
    "validation_log_mse = [np.log(i) for i in training_history.history['val_mse']]\n",
    "plt.plot(train_log_mse)\n",
    "plt.plot(validation_log_mse)\n",
    "plt.title('Model Loss on training set and validation set')\n",
    "plt.ylabel('log(MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last step is to save the model in the ./model/output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.getcwd() + \"/model/output/\" + \"Heston_NN_model.h5\"\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
